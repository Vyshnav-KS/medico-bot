{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings()\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# def prompt_router(input):\n",
    "#     query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "#     similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "#     most_similar = prompt_templates[similarity.argmax()]\n",
    "#     print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "#     return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "# chain = (\n",
    "#     {\"query\": RunnablePassthrough()}\n",
    "#     | RunnableLambda(prompt_router)\n",
    "#     | AzureChatOpenAI()\n",
    "#     | StrOutputParser()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    output: str = Field(description=\"Answer to the user query.\")\n",
    "    follow_up: List[str] = Field(description=\"List of 4 follow up questions related to query and output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'Myopia, also known as nearsightedness, is a common vision condition in which you can see objects near to you clearly, but objects farther away are blurry. It occurs when the shape of your eye causes light rays to bend (refract) incorrectly, focusing images in front of your retina instead of on your retina.',\n",
       " 'follow_up': ['What causes myopia?',\n",
       "  'How is myopia diagnosed?',\n",
       "  'What are the treatment options for myopia?',\n",
       "  'Can myopia be prevented or its progression slowed?']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "m_query = \"What is Myopia ?\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": m_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\Desktop\\Projects\\venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.outputs import LLMResult\n",
    "from chain.response_generator import get_response\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler\n",
    "from typing import Any\n",
    "from flask_socketio import send, emit\n",
    "import time\n",
    "from flask import Flask\n",
    "from flask_socketio import SocketIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallbackHandler(FinalStreamingStdOutCallbackHandler):\n",
    "    def __init__(self):\n",
    "        # Initialize instance variables instead of using global variables\n",
    "        self.content_checker = True\n",
    "        self.start_stream = False\n",
    "        self.check_colon = False\n",
    "        self.content = \"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "        if self.content_checker:\n",
    "            self.content += token\n",
    "\n",
    "        if self.content_checker and \"Final\" in self.content:\n",
    "            self.check_colon = True\n",
    "            self.content = \"\"\n",
    "\n",
    "        if self.check_colon and \":\" in self.content:\n",
    "            self.content_checker = False\n",
    "            self.start_stream = True\n",
    "            self.check_colon = False\n",
    "            self.content = \"\"\n",
    "            return\n",
    "            \n",
    "        if self.start_stream:\n",
    "            return token\n",
    "            # send_ai_res(token)\n",
    "            # print(token)\n",
    "        # send_ai_res(token)\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        # Reset instance variables at the end of the response\n",
    "        self.content_checker = True\n",
    "        self.start_stream = False\n",
    "        self.content = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOP\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def p():\n",
    "    print(\"NOP\")\n",
    "\n",
    "c = asyncio.create_task(p())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running medical tool\n",
      "Similarity search initiated...\n",
      "Final content: association between \n",
      ": Myopia, commonly known as nearsightedness, is a condition where the eye is longer than normal or the cornea is too curved, causing light rays to focus in front of the retina instead of on it. This results in distant objects appearing blurry while close objects can be seen clearly. The development of myopia can involve complex interactions between genetic and environmental factors. Recent research suggests that vitamin A supplementation may not have an independent effect on myopia, but intraocular processes associated with vitamin A could indirectly contribute to its development. Moreover, variations in gene expression, such as BMP2 gene variations, have been implicated in ocular development and may contribute to high myopia. Particularly, the gene BMP2-inducible kinase (BMP2K, BIKe) has been found to be upregulated during BMP2-induced osteoblast differentiation and may be connected to susceptibility to high myopia, where individuals have a spherical equivalent greater than -6.00 diopters[1](https://pubmed.ncbi.nlm.nih.gov/19927351/).\n",
      "\n",
      "Given this information, if you are interested in learning more about myopia and related factors, you may consider asking about:\n",
      "\n",
      ">- [**Q1**] How does vitamin A and its intraocular processes contribute to the development of myopia?\n",
      ">- [**Q2**] What is the role of BMP2 gene expression in ocular development and myopia?\n",
      ">- [**Q3**] Can you provide more details on the relationship between gut microbiota and ocular health?\n",
      ">- [**Q4**] What are the latest findings on genetic factors contributing to high myopia?Error in callback: cannot access local variable 'tok' where it is not associated with a value\n"
     ]
    }
   ],
   "source": [
    "handler_1 = MyCallbackHandler()\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "socketio = SocketIO(app, cors_allowed_origins=\"*\")\n",
    "\n",
    "def run_test_prompt(input_text):\n",
    "    executor = get_response()\n",
    "    executor.invoke({\n",
    "            \"input\": input_text,\n",
    "            \"chat_history\": [],\n",
    "        },\n",
    "        {\"callbacks\": [handler_1]},\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        for tok in handler_1.on_llm_new_token():\n",
    "            print(tok, end='', flush=True)  # Print without adding a newline, flush to force immediate printing\n",
    "            yield tok\n",
    "    except Exception as e:\n",
    "        print(f\"Error in callback: {e}\")\n",
    "\n",
    "for output in run_test_prompt(\"What is Myopia?\"):\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_ai_res(msg):\n",
    "    emit('ai_res', msg, broadcast=False, ignore_queue=False)\n",
    "    socketio.sleep(0)\n",
    "\n",
    "@socketio.on('start_stream')\n",
    "def handle_messagex(data):\n",
    "    print('received message: ' + data)\n",
    "    emit(\"message\", \"this is a message from the server\")\n",
    "    socketio.sleep(0)\n",
    "    # run_test_prompt(\"What is Myopia?\")\n",
    "\n",
    "@socketio.on('send_message')\n",
    "def handle_message(data):\n",
    "    print('received message from client: ' + data)\n",
    "    run_test_prompt(data)\n",
    "\n",
    "socketio.run(app, port=5001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
